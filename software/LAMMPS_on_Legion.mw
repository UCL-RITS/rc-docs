
[[Category:Software]][[Category:Applications]]
<table style="width:37%; background-color:#F9F9F9;float:right;margin-left:10px;">
<tr>
<td style="vertical-align:top;">
'''Quick Links'''
<ul style="column-count:2;-moz-column-count:2;-webkit-column-count:2">
<li>[[RC Systems | Platforms Overview]]</li>
<li>[[Additional Resource Requests| Request Additional Resources]]</li>
<li>[[:Category:User Guide | User Guide]]</li>
<li>[[Software| Software]]</li>
<li>[[Legion Metrics | Service Metrics]]</li>
<li>[[Account Services]]</li>
</ul>
</td>
</tr>
<tr>
<td style="vertical-align:top;">
__NOEDITSECTION__[[Contact and Support|Contact & Support]]<br />
For support for any of our services or for general advice and consultancy, email:<br />
[mailto:rc-support@ucl.ac.uk rc-support@ucl.ac.uk]

</td>
</tr>
</table>


[[Applications | Back to Applications]]<br /><br />
'''Current Version: 10Feb15, 8Dec15'''<br />
LAMMPS is an open source parallel molecular dynamics code which exhibits good scaling in a wide range of environments. <br /><br />'''Useful Links:'''

* [http://lammps.sandia.gov/ LAMMPS website]    <br /><br />
==Setup==
LAMMPS-8Dec15 was built with additional packages kspace, replica, rigid, and class2. You can access the binaries on Grace or Legion with <code>lmp_default</code> which is a symlink to <code>lmp_legion</code> or <code>lmp_grace</code> respectively. 

The 10Feb15 version does not have a class2 package. <br /> <br />The following changes need to be made to the standard modules before running this software: <br />

<code>module load lammps/8Dec15/intel-2015-update2</code><br />
==Example Job Submission Script==

<div style="background-color:#F9F9F9;width=100%;padding:5px 10px 5px 10px;">
<code>
 <nowiki>
#!/bin/bash -l

# Batch script to run an MPI parallel job on Legion with the upgraded software
# stack under SGE with Intel MPI. Updated Jan 2016.

# 1. Force bash as the executing shell.
#$ -S /bin/bash

# 2. Request one hour of wallclock time (format hours:minutes:seconds).
#$ -l h_rt=1:00:00

# 3. Request 1 gigabyte of RAM per process.
#$ -l mem=1G

# 4. Set the name of the job.
#$ -N ExampleLAMMPS

# 5. Select the MPI parallel environment and 24 processes.
#$ -pe mpi 24

# 7. Set the working directory to somewhere in your scratch space.  This is
# a necessary step with the upgraded software stack as compute nodes cannot
# write to $HOME.
# Replace "<your_UCL_id>" with your UCL user ID.
#$ -wd /home/<your_UCL_id>/Scratch/lammps

# 8. Load required module alongside default modules

module load lammps/8Dec15/intel-2015-update2

# 9. Run our MPI job.  Replace "inputfile" with the name of your LAMMPS input file.
gerun `which lmp_default` -in inputfile
</nowiki>
</code>
</div>



[[#top | back to top]]






