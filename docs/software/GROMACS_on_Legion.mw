
[[Category:Software]][[Category:Applications]]
<table style="width:37%; background-color:#F9F9F9;float:right;margin-left:10px;">
<tr>
<td style="vertical-align:top;">
'''Quick Links'''
<ul style="column-count:2;-moz-column-count:2;-webkit-column-count:2">
<li>[[RC Systems | Platforms Overview]]</li>
<li>[[Additional Resource Requests| Request Additional Resources]]</li>
<li>[[:Category:User Guide | User Guide]]</li>
<li>[[Software| Software]]</li>
<li>[[Legion Metrics | Service Metrics]]</li>
<li>[[Account Services]]</li>
</ul>
</td>
</tr>
<tr>
<td style="vertical-align:top;">
__NOEDITSECTION__[[Contact and Support|Contact & Support]]<br />
For support for any of our services or for general advice and consultancy, email:<br />
[mailto:rc-support@ucl.ac.uk rc-support@ucl.ac.uk]

</td>
</tr>
</table>


[[Applications | Back to Applications]]<br /><br />
'''Current Version: 5.0.4'''<br />
GROMACS is an open source molecular dynamics package. Both double and single precision builds of GROMACS are provided. 

==GROMACS versions==
There are several versions of GROMACS available on Legion, built with different module combinations. You can see the prerequisite modules with <code>module show <modulename></code>.

Which executable you should run depends on the problem you wish to solve.  For both single and double precision version builds, serial binaries and an MPI binary for mdrun (mdrun_mpi) are provided. Double precision binaries have a _d suffix (so mdrun_d, mdrun_mpi_d etc). <br /><br />'''Useful Links:'''

* [http://www.gromacs.org/ GROMACS web site]


[[Category:Pages with bash scripts]]
==GROMACS 5.0.4==
GROMACS 5.0.4 was built with the Intel 2015 compilers, MKL and Intel MPI.
<div style="background-color:#F9F9F9;width=100%;padding:5px 10px 5px 10px;"><code>
 <nowiki>
#!/bin/bash -l

# Batch script to run an MPI parallel GROMACS job on Legion with the upgraded software
# stack under SGE with Intel MPI. Updated Oct 2015.

# 1. Force bash as the executing shell.
#$ -S /bin/bash

# 2. Request ten minutes of wallclock time (format hours:minutes:seconds).
#$ -l h_rt=0:10:0

# 3. Request 1 gigabyte of RAM per process.
#$ -l mem=1G

# 4. Request 15 gigabyte of TMPDIR space per node (default is 10 GB)
#$ -l tmpfs=15G

# 5. Set the name of the job.
#$ -N GROMACS_1_16

# 6. Select the MPI parallel environment and 16 processes.
#$ -pe mpi 16

# 7. Set the working directory of the job to the current directory
#     containing your input files.
#    This *has* to be somewhere in your Scratch space, or else your
#     job will go into the Eqw state.
#$ -cwd

# Using the default compiler and MPI
module load gromacs/5.0.4/intel-2015-update2

# Run GROMACS - replace with mdrun command line suitable for your job!

gerun mdrun_mpi -v -stepout 10000
</nowiki>
</code>
</div>






[[Category:Pages with bash scripts]]
==GROMACS 5.0.4 with Plumed==
GROMACS 5.0.4 was built with Plumed support, the Intel 2015 compilers, OpenBLAS and Intel MPI. (Plumed did not work correctly with MKL).
<div style="background-color:#F9F9F9;width=100%;padding:5px 10px 5px 10px;"><code>
 <nowiki>
#!/bin/bash -l

# Batch script to run an MPI parallel GROMACS job on Legion with the upgraded software
# stack under SGE with Intel MPI. Updated Oct 2015.

# 1. Force bash as the executing shell.
#$ -S /bin/bash

# 2. Request ten minutes of wallclock time (format hours:minutes:seconds).
#$ -l h_rt=0:10:0

# 3. Request 1 gigabyte of RAM per process.
#$ -l mem=1G

# 4. Request 15 gigabyte of TMPDIR space per node (default is 10 GB)
#$ -l tmpfs=15G

# 5. Set the name of the job.
#$ -N GROMACS_1_16_plumed

# 6. Select the MPI parallel environment and 16 processes.
#$ -pe mpi 16

# 7. Set the working directory of the job to the current directory
#     containing your input files.
#    This *has* to be somewhere in your Scratch space, or else your
#     job will go into the Eqw state.
#$ -cwd

# Using default compilers and MPI
module load openblas/0.2.14/intel-2015-update2
module load plumed/2.1.2/intel-2015-update2
module load gromacs/5.0.4/plumed/intel-2015-update2

# Run GROMACS - replace with mdrun command line suitable for your job!

gerun mdrun_mpi -v -stepout 10000
</nowiki>
</code>
</div>





==Checkpoint and restart==

GROMACS has built-in checkpoint and restart ability, so you can use this if your runs will not complete in the maximum 48hr wallclock time.

Have a look at the GROMACS manual for full details, as there are more options than mentioned here.

You can tell GROMACS to write a checkpoint file when it is approaching the maximum wallclock time available, and then exit.

In this case, we had asked for 48hrs wallclock. This tells GROMACS to start from the last checkpoint if there is one, and write a new checkpoint just before it reaches 47 hrs runtime.

<code>
 gerun mdrun_mpi -cpi -maxh 47 <options>
</code>

The next job you submit with the same script will carry on from the checkpoint the last job wrote. You could use job dependencies to submit two identical jobs at the same time and have one dependent on the other, so it won't start until the first finishes - have a look at <code>man qsub</code> for the <code>-hold_jid</code> option.

Alternatively, you can tell it to write checkpoints at set intervals and get it to automatically resubmit your job if it runs out of time. Modify your jobscript to include the below. It will write checkpoints every 15 mins if you don't specify a time.

<code>
 # This SGE option means a signal is sent to the job shortly before the scheduler kills it.
 #$ -notify

 # This catches the signal and resubmits your job to the queue after it has run out of time.
 # When your job has properly finished, that is a signal 0 so it won't be restarted again.
 trap 'exit 99' SIGUSR2

 # load your modules as normal here
 ...

 # Write checkpoints every 120 mins, start from checkpoint if there is one.
 gerun mdrun_mpi -cpi -cpt 120 <options>

</code>    <br /><br />








