
[[Category:Software]][[Category:Applications]]
<table style="width:37%; background-color:#F9F9F9;float:right;margin-left:10px;">
<tr>
<td style="vertical-align:top;">
'''Quick Links'''
<ul style="column-count:2;-moz-column-count:2;-webkit-column-count:2">
<li>[[RC Systems | Platforms Overview]]</li>
<li>[[Additional Resource Requests| Request Additional Resources]]</li>
<li>[[:Category:User Guide | User Guide]]</li>
<li>[[Software| Software]]</li>
<li>[[Legion Metrics | Service Metrics]]</li>
<li>[[Account Services]]</li>
</ul>
</td>
</tr>
<tr>
<td style="vertical-align:top;">
__NOEDITSECTION__[[Contact and Support|Contact & Support]]<br />
For support for any of our services or for general advice and consultancy, email:<br />
[mailto:rc-support@ucl.ac.uk rc-support@ucl.ac.uk]

</td>
</tr>
</table>


[[Applications | Back to Applications]]<br /><br />
'''Current Version: Revisions: D.01'''<br />
Gaussian 09 (G09) revision D.01 is available on Legion and can be used in either:

* Serial mode and shared memory parallel mode within single nodes using at most twelve processors when running on type X, Y or Z nodes. Module: ''gaussian/g09-d01/pgi-2015.4''
* Linda parallel execution mode across multiple nodes. Note: not all calculations are Linda parallel. From the Gaussian documentation "HF, CIS=Direct, and DFT calculations on molecules are Linda parallel, including energies, optimizations and frequencies. TDDFT energies and MP2 energies and gradients are also Linda parallel. PBC calculations are not Linda parallel. The default for molecules larger than 65 atoms is to use the linear scaling algorithms (FMM). The linear scaling (FMM-based) algorithms are now Linda-parallel, so Linda parallel jobs on large molecules do not need to specify NoFMM, and will run faster with the default algorithms chosen by the program." 

Access to G09 is enabled by a module file and being a member of the appropriate reserved application group. Please email [mailto:rc-support@ucl.ac.uk rc-support@ucl.ac.uk] to get your userid added to the G09 group. <br /><br />'''Useful Links:'''

* [http://www.gaussian.com/g_tech/g_ur/g09help.htm Gaussian 09 User's Reference documentation on the Gaussian Inc website.]    <br /><br />
==Setup==
The following changes need to be made to the standard modules before running this software: <br />

<code>module load gaussian/g09-d01/pgi-2015.4</code><br /><br />All G09 jobs apart from small test jobs (4 cores and less than 5 minutes runtime) must be submitted as batch jobs. Before you can run G09 interactively you need to load the G09 module and run an initialisation script using: 
<code>
 . $g09root/g09/bsd/g09.profile
</code>
You can use:
<code>
 module list
</code>
to check that the module is loaded. 
You should now be able to run G09 using:
<code>
 g09 < myG09input > myG09output
</code>
for example.

To submit batch jobs to the cluster you will need a runscript.
==Shared memory Gaussian jobs==
Here is a simple example G09 runscript for shared memory jobs including comments:
<div style="background-color:#F9F9F9;width=100%;padding:5px 10px 5px 10px;">
<code>
 <nowiki>
#!/bin/bash -l
# Batch script to run a shared memory Gaussian 09 job on Legion with the
# upgraded software stack under SGE.
#
# Oct 2015
#
# Based on openmp.sh by:
#
# Owain Kenway, Research Computing, 16/Sept/2010

#$ -S /bin/bash

# 1. Request 12 hours of wallclock time (format hours:minutes:seconds).
#$ -l h_rt=12:0:0

# 2. Request 2 gigabytes of RAM per thread.
#$ -l mem=2G

# 3. Request 10 gigabytes of TMPDIR space (default is 10 GB). $GAUSS_SCRDIR is in here.
#$ -l tmpfs=10G

# 4. Set the name of the job.
#$ -N G09_job1

# 5. Select  4 OpenMP threads
#$ -pe smp 4

# 6. Set the working directory to somewhere in your scratch space.  This is
# a necessary step with the upgraded software stack as compute nodes cannot
# write to $HOME.
#
# Note: this directory MUST exist before your job starts!
#$ -wd /home/<your_UCL_id>/Scratch/G09_output

# Setup G09 runtime environment
module load gaussian/g09-d01/pgi-2015.4
source $g09root/g09/bsd/g09.profile
mkdir -p $GAUSS_SCRDIR

# Run g09 job
echo "GAUSS_SCRDIR = $GAUSS_SCRDIR"
echo ""
echo "Running: g09 < $g09infile > $g09outfile"
g09 < $g09infile > $g09outfile
</nowiki>
</code>
</div>

Please copy if you wish and edit it to suit your jobs. You will need to change the ''-wd /home/<your_UCL_id>/Scratch/G09_output'' SGE directives and may need to change the memory, wallclock time, number of threads and job name directives as well. A suitable qsub command to submit a G09 job using this runscript would be:
<code>
 qsub -v g09infile=`pwd`/MyData.com,g09outfile=MyOutput.out run-g09.sh
</code>
where ''Mydata.com'' is the file containing your G09 commands and ''MyOuput.out'' is the output file. In this example input, and your runscript files are in your current working directory. The output file is saved in the directory specified by the -wd SGE directive.

[[#top | back to top]]
==Linda parallel Gaussian jobs==
Here is a simple example G09 runscript for Linda parallel jobs including comments:
<div style="background-color:#F9F9F9;width=100%">
<code>
 <nowiki>
#!/bin/bash -l

# Batch script to run a Gaussian 09 job on Legion using Linda with the upgraded
# software stack under SGE.
#
# Oct 2015
#
# Based on the MPI and Hybrid example scripts

#$ -S /bin/bash

# 2. Request 12 hours of wallclock time (format hours:minutes:seconds).
#$ -l h_rt=12:0:0

# 3. Request 4 gigabyte of RAM per core.
#$ -l mem=2G

# 4. Request 10 gigabytes of TMPDIR space (default is 10 GB). $GAUSS_SCRDIR is in here.
#$ -l tmpfs=10G

# 5. Set the name of the job.
#$ -N G09l_job1

# 6. Select the MPI parallel environment and 8 cores total
#$ -pe mpi 8

# 7. Set the working directory to somewhere in your scratch space.  This is
# a necessary step with the upgraded software stack as compute nodes cannot
# write to $HOME.
#
# Note: this directory MUST exist before your job starts!
#$ -wd /home/<your_UCL_id>/Scratch/G09_output

# 8. Select number of threads per Linda worker (value of NProcShared in your
#     Gaussian input file. This will give 8/4 = 2 Linda workers.
export OMP_NUM_THREADS=4

# 9. Run our G09 Linda job.

# Setup G09 runtime environment

module load gaussian/g09-d01/pgi-2015.4
source $g09root/g09/bsd/g09.profile
mkdir -p $GAUSS_SCRDIR

# Pre-process G09 input file to include nodes allocated to job

echo "Running: lindaConv $g09infile $JOB_ID $TMPDIR/machines"
echo ''
$lindaConv $g09infile $JOB_ID $TMPDIR/machines

# Run g09 job

echo "GAUSS_SCRDIR = $GAUSS_SCRDIR"
echo ""
echo "Running: g09 < job$JOB_ID.com > $g09outfile"

# communication needs to be via ssh not the Linda default
export GAUSS_LFLAGS='-v -opt "Tsnet.Node.lindarsharg: ssh"'

time g09 < job$JOB_ID.com > $g09outfile
</nowiki>
</code>
</div>

This script is more complicated that the shared memory example as your Gaussian input file needs to be preprocessed to insert information about the nodes SGE has allocated to the job.

Please copy if you wish and edit it to suit your jobs. You will need to change the ''-wd /home/<your_UCL_id>/Scratch/G09_output'' SGE directive and may need to change the memory, wallclock time, number of total cores (the -pe directive), number of threads and job name directives as well. A suitable qsub command to submit a G09 job using this runscript would be:
<code>
 qsub -v g09infile=`pwd`/MyData.com,g09outfile=MyOutput.out run-g09-linda.sh
</code>

where ''Mydata.com'' is the file containing your G09 commands and ''MyOuput.out'' is the output file. In this example input, and your runscript files are in your current working directory. The output file is saved in the directory specified by the -wd SGE directive.

==Submitting Long Gaussian Jobs==

It is possible to obtain permission to submit single node Gaussian jobs with wallclock times between 2 and 7 days. To submit jobs of this type you will have to apply to the CRAG - see [[Legion Resource Allocation#Requests for Additional Resources or Resource Reservations | Requests for Additional Resources]].

==Memory Errors==

If you encounter errors like:

<code>
Out-of-memory error in routine ShPair-LoodLd2 (IEnd=        257724 MxCore=        242934)

Use %mem=48MW to provide the minimum amount of memory required to complete this step.
</code>

Try adding this to your jobscript:

<code>
export GAUSS_MEMDEF=48000000
</code>

You may need to increase this value even more to allow it to run.

==No space left on device==

If you get this error
<code>
  g_write: No space left on device
</code>

The $GAUSS_SCRDIR is probably full - you should increase the amount of tmpfs you are requesting.

[[#top | back to top]]





