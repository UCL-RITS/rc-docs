
[[Category:Software]][[Category:Applications]]
<table style="width:37%; background-color:#F9F9F9;float:right;margin-left:10px;">
<tr>
<td style="vertical-align:top;">
'''Quick Links'''
<ul style="column-count:2;-moz-column-count:2;-webkit-column-count:2">
<li>[[RC Systems | Platforms Overview]]</li>
<li>[[Additional Resource Requests| Request Additional Resources]]</li>
<li>[[:Category:User Guide | User Guide]]</li>
<li>[[Software| Software]]</li>
<li>[[Legion Metrics | Service Metrics]]</li>
<li>[[Account Services]]</li>
</ul>
</td>
</tr>
<tr>
<td style="vertical-align:top;">
__NOEDITSECTION__[[Contact and Support|Contact & Support]]<br />
For support for any of our services or for general advice and consultancy, email:<br />
[mailto:rc-support@ucl.ac.uk rc-support@ucl.ac.uk]

</td>
</tr>
</table>


[[Applications | Back to Applications]]<br /><br />
'''Current Version: 9.06.011'''<br />
StarCCM+ is a commercial CFD package that handles fluid flows, heat transfer, stress simulations, and other common applications of such. <br /><br />
==Setup==
You must request access to the group controlling StarCCM+ access (legstarcd) to use it.

StarCCM+ is intended to be run primarily within batch jobs however you may run short (less than 15 minutes execution time) interactive work on the Login Nodes and longer (up to two hours) on the User Test Nodes. Interactive work can be done using the full  GUI provided you have X-windows functionality enabled though your ssh connection. See our [[:Category:Legion User Guide | Legion User Guide]] for more information about enabling X-windows functionality and the User Test nodes. <br /> <br />You will also need to load the appropriate OpenMPI module, and the StarCCM+ module: <br />

<code>module unload mpi</code><br /><code>module load mpi/openmpi/1.8.4/intel-2015-update2</code><br /><code>module load star-ccm+/9.06.011</code><br /><br />Before running any StarCCM+ jobs on the cluster you '''MUST''' load the StarCCM+ module on a login node. This is so the module can  set up two symbolic links in your home directory to directories  created in your Scratch area. This is so that users settings etc can be written by running jobs.
[[Category:Pages with bash scripts]]
==Example StarCCM+ runscript==
Here is an example run script for submitting batch jobs to the cluster:
<div style="background-color:#F9F9F9;width=100%;padding:5px 10px 5px 10px;"><code>
 <nowiki>
#!/bin/bash -l

# Batch script to run a parallel STAR-CCM+ job on Legion with the upgraded
# software stack under SGE. This version works with the modules
# environment upgraded in Oct 2015.

# STAR-CCM+ Version 9.06.011

# Submit using a qsub command of the form:
#
#   qsub -v simFile=`pwd`/my_input_file run-starccm+.sh
#

# 1. Force bash as the executing shell.
#$ -S /bin/bash

# 2. Request fifteen hours of wallclock time (format hours:minutes:seconds).
#    Change this to suit your requirements.
#$ -l h_rt=15:0:0

# 3. Request 2 gigabyte of RAM. Change this to suit your requirements.
#$ -l mem=2G

# 4. Set the name of the job. You can change this if you wish.
#$ -N StarCCM+_Job1

# 5. Merge output and error files
#$ -j y

# 6. Request MPI environment with 16 processors
#$ -pe mpi 16

# 7. Request 1 license per core
#$ -l ccmpsuite=1

# 9. Run from the current working directory
#$ -cwd

# 10. Load modules needed for STAR-ccm+
module unload mpi
module load mpi/openmpi/1.8.4/intel-2015-update2
module load star-ccm+/9.06.011

starccm+ -np $NSLOTS -machinefile $TMPDIR/machines -rsh ssh -batch $simFile
</nowiki>
</code>
</div>

Please copy if you wish and edit it to suit your jobs. The script can then be submitted using a qsub command like:
<code>
 qsub -v simFile=`pwd`/my_input.sim run-starccm+.sh
</code>
Output will be written to the current working directory, so this should be submitted from a directory in the Scratch area. 
[[#top | back to top]]






