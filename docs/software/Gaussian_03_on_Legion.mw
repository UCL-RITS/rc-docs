
[[Category:Software]][[Category:Applications]]
<table style="width:37%; background-color:#F9F9F9;float:right;margin-left:10px;">
<tr>
<td style="vertical-align:top;">
'''Quick Links'''
<ul style="column-count:2;-moz-column-count:2;-webkit-column-count:2">
<li>[[RC Systems | Platforms Overview]]</li>
<li>[[Additional Resource Requests| Request Additional Resources]]</li>
<li>[[:Category:User Guide | User Guide]]</li>
<li>[[Software| Software]]</li>
<li>[[Legion Metrics | Service Metrics]]</li>
<li>[[Account Services]]</li>
</ul>
</td>
</tr>
<tr>
<td style="vertical-align:top;">
__NOEDITSECTION__[[Contact and Support|Contact & Support]]<br />
For support for any of our services or for general advice and consultancy, email:<br />
[mailto:rc-support@ucl.ac.uk rc-support@ucl.ac.uk]

</td>
</tr>
</table>


[[Applications | Back to Applications]]<br /><br />
'''Current Version: Revision: E.01'''<br />
Gaussian 03 (G03) revision E.01 is available on legion and currently can be used in serial mode and shared memory parallel mode within single nodes using at most four processors. Access to G03 is enabled by a module file and being a member of the appropriate reserved application group. Please email [mailto:rc-support@ucl.ac.uk rc-support@ucl.ac.uk] to get your userid added to the G03 group. <br /><br />
==Setup==
The following changes need to be made to the standard modules before running this software: <br />

<code>module load gaussian/g03_e01/pgi</code><br /><br />All G03 jobs apart from small test jobs (4 cores and less than 5 minutes runtime) must be submitted as batch jobs. Before you can run G03 interactively you need to load the G03 module and run an initialisation script using:
<code>
 . $g03root/g03/bsd/g03.profile
</code>
You can use:
<code>
 module list
</code>
to check that the module is loaded. Output should look similar to this:
<code>
 Currently Loaded Modulefiles:
  1) ucl                          6) nedit/5.6
  2) compilers/intel/11.1/072     7) mrxvt/0.5.4
  3) mkl/10.2.5/035               8) rcops/1.0
  4) mpi/qlogic/1.2.7/intel       9) gaussian/g03_e01/pgi
  5) sge/6.2u3
</code>
You should now be able to run G03 using:
<code>
g03 < myG03input > myG03output
</code>
for example.
==Example Job Submission Script==
To submit batch jobs to the cluster you will need a runscript. Here is a simple example G03 runscript for shared memory jobs including comments:
<div style="background-color:#F9F9F9;width=100%;padding:5px 10px 5px 10px;">
<code>
 <nowiki>
#!/bin/bash -l

# Batch script to run a Gaussian 03 job on Legion with the upgraded
# software stack under SGE.
#
# 21st Sept 2010
#
# Based on openmp.sh by:
#
# Owain Kenway, Research Computing, 16/Sept/2010

#$ -S /bin/bash

# 1. Request 12 hours of wallclock time (format hours:minutes:seconds).
#$ -l h_rt=12:0:0

# 2. Request 4 gigabyte of RAM.
#$ -l mem=4G

# 3. Set the name of the job.
#$ -N G03_job1

# 4. Select  4 OpenMP threads (the most possible on Legion).
#$ -l thr=4

# 6. Set the working directory to somewhere in your scratch space.  This is
# a necessary step with the upgraded software stack as compute nodes cannot
# write to $HOME.
# Replace "<your_UCL_id>" with your UCL user ID :)
#$ -wd /home/<your_UCL_id>/Scratch/G03_output

#Old PBS setting: -N G03_job1
#Old PBS setting: -l nodes=1:ppn=4,naccesspolicy=singlejob,qos=parallel
#Old PBS setting: -l pvmem=8g,walltime=12:00:00
#Old PBS setting: -A ucl/<consortium name>/<project name>

# Setup G03 runtime environment

module load gaussian/g03_e01/pgi
mkdir -p $GAUSS_SCRDIR
. $g03root/g03/bsd/g03.profile

echo "GAUSS_SCRDIR = $GAUSS_SCRDIR"
echo ""
echo "Running: g03 < $g03infile > $g03outfile"
g03 < $g03infile > $g03outfile
</nowiki>
</code>
</div>
This is available on:
<code>
 /shared/ucl/apps/Gaussian/G03_E01/run-g03.sh
</code>
Please copy if you wish and edit it to suit your jobs. You will need to change the ''-wd /home/<your_UCL_id>/Scratch/G03_output'' SGE directives and may need to change the memory, wallclock time, number of threads and job name directives as well. A suitable qsub command to submit a G03 job using this runscript would be:
<code>
 qsub -v g03infile=`pwd`/MyData.com,g03outfile=MyOutput.out run-g03.sh
</code>
where ''Mydata.com'' is the file containing your G03 commands and ''MyOuput.out'' is the output file. In this example input, and your runscript files are in your current working directory. The output file is saved in the directory specified by the -wd SGE directive.

[[#top | back to top]]
==Submitting Long Gaussian Jobs==
It is possible to obtain permission to submit single node Gaussian jobs with wallclock times between 2 and 7 days. For details of how to gain access to the 7-day Gaussian queue see [[Legion Resource Allocation#Requests for Additional Resources or Resource Reservations | Requests for Additional Resources]].

As the 7-day queue is restricted to shared memory Gaussian jobs you will need to make some changes to your runscripts:
# Include the grid engine directive:<br /><code>#$ -ac app=g03</code><br />for Gaussian 03 jobs. If the directive is not present, normal job wallclock limits apply.
# The way Gaussian is launched needs to be modified as the new queues launch g03 via a new wrapper command. The new wrapper is G03  - note the capital G! It takes arguments for Gaussian command (standard input) and output (standard output) files so need to be used like so:<br /><code>G03 commands.in output.out</code><br />where commands.in is the file containing your Gaussian commands and output.out is the file where standard output will appear. The G09 wrapper is used in the same way.

Here is a simple Gaussian 03 runscript using the new '7-day' queue:
<div style="background-color:#F9F9F9;width=100%">
<code>
 <nowiki>
#!/bin/bash -l

# Batch script to run a Gaussian 03 job on Legion using 
# the restricted '7-day' queue under SGE 
#
# Aug 2012
#
# Based on openmp.sh by:
#
# Owain Kenway, Research Computing, 16/Sept/2010

#$ -S /bin/bash

# 1. Request 12 hours of wallclock time (format hours:minutes:seconds).
#$ -l h_rt=3:0:0

# 2. Request 4 gigabyte of RAM.
#$ -l mem=8G

#$ -ac app=g03

# 3. Set the name of the job.
#$ -N G03_jobR

# 4. Select  12 OpenMP threads (the most possible on Legion).
#$ -l thr=12

# 6. Set the working directory to somewhere in your scratch space.  This is
# a necessary step with the upgraded software stack as compute nodes cannot
# write to $HOME.
#
# Note: this directory MUST exist before your job starts!
#$ -wd /home/<your_UCL_id>/Scratch/G03_output

# Run g03 job
echo "GAUSS_SCRDIR = $GAUSS_SCRDIR"
echo ""
echo "Running: G03 $g03infile $g03outfile"
time G03 $g03infile $g03outfile
</nowiki>
</code>
</div>
This is available on:
<code>
 /shared/ucl/apps/Gaussian/G03_E01/run-g03-res.sh
</code>
Please copy if you wish and edit it to suit your jobs. You will need to change the ''-wd /home/<your_UCL_id>/Scratch/G03_output'' SGE directive and may need to change the memory, wallclock time, number of threads and job name directives as well. A suitable qsub command to submit a G03 job using this runscript would be:
<code>
 qsub -v g03infile=`pwd`/MyData.com,g03outfile=MyOutput.out run-g03-res.sh
</code>
where ''Mydata.com'' is the file containing your G03 commands and ''MyOuput.out'' is the output file. In this example input, and your runscript files are in your current working directory. The output file is saved in the directory specified by the -wd SGE directive.

[[#top | back to top]]





